{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from t2e_utils import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "palette=sns.color_palette(\"RdBu_r\", 50)\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "import wtte.wtte as wtte\n",
    "from keras.models import Sequential, load_model,Model\n",
    "from keras.layers import Dense,LSTM,GRU,Activation,Masking,BatchNormalization,Lambda,Input\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "from keras.optimizers import RMSprop,adam,Nadam\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "plt.style.use('seaborn-muted')\n",
    "# # np.random.seed(2)\n",
    "# # pd.set_option(\"display.max_rows\",1000)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Tax_paper/code/output_files/results/'\n",
    "a = os.path.join(path,'suffix_and_remaining_time_helpdesk.csv')\n",
    "b = os.path.join(path,'suffix_and_remaining_time_bpi_12_w.csv')\n",
    "c = os.path.join(path,'suffix_and_remaining_time_bpi_12_w_no_repeat.csv')\n",
    "d = os.path.join(path,'suffix_and_remaining_time_env_permit.csv')\n",
    "\n",
    "a = pd.read_csv(a)\n",
    "b = pd.read_csv(b)\n",
    "c = pd.read_csv(c)\n",
    "d = pd.read_csv(d)\n",
    "\n",
    "df_dict = {\n",
    "    'a': a,\n",
    "    'b': b,\n",
    "    'c': c,\n",
    "    'd': d\n",
    "}\n",
    "range_dict = {\n",
    "    'a': range(2,8,1),\n",
    "    'b': range(2,22,2),\n",
    "    'c': range(2,12,2),\n",
    "    'd': range(2,22,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(case):\n",
    "    endtime = case[\"CompleteTimestamp\"].reset_index(drop=True)[len(case)-1]\n",
    "    starttime = case[\"CompleteTimestamp\"].reset_index(drop=True)[0]\n",
    "    case[\"fvt1\"] = case[\"CompleteTimestamp\"].diff(periods=1)\n",
    "    case[\"fvt2\"] = case[\"CompleteTimestamp\"].dt.hour\n",
    "    case[\"fvt3\"] = case[\"CompleteTimestamp\"].dt.weekday*24 + case[\"CompleteTimestamp\"].dt.hour\n",
    "    case[\"T2E\"] =  endtime - case[\"CompleteTimestamp\"]\n",
    "    return case\n",
    "\n",
    "def xy_split(processed_dataset, resolution = 'hourly',n_steps=2):\n",
    "    X = processed_dataset.groupby([\"CaseID\"]).apply(lambda df:extract_X(df, n_steps))\n",
    "    X = np.concatenate(X.values)\n",
    "    y = processed_dataset.groupby([\"CaseID\"]).apply(lambda df:extract_y(df, n_steps, resolution = resolution))\n",
    "    y = np.concatenate(y.values)\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "    y = np.append(y,np.ones_like(y),axis=1)\n",
    "    return X,y\n",
    "\n",
    "def extract_X(df,n_steps):\n",
    "    feature_idx = np.concatenate(\\\n",
    "               np.where(df.columns.str.contains('ActivityID_')) + \\\n",
    "#                np.where(df_helpdesk_preprocessed.columns.str.contains('weekday_')) + \\\n",
    "               np.where(df.columns.str.contains('fvt'))\n",
    "              )\n",
    "    x = []\n",
    "    steps = len(df)-n_steps+1\n",
    "    steps = 1\n",
    "    for i in range(steps):\n",
    "        x.append(df.values[i:i+n_steps,feature_idx])\n",
    "    return np.array(x)\n",
    "\n",
    "def extract_y(df,n_steps,resolution='hourly'):\n",
    "    y = []\n",
    "    if resolution == 'hourly':\n",
    "        time_idx = np.where(df.columns.str.contains(\"H2E\"))[0]\n",
    "    else:\n",
    "        time_idx = np.where(df.columns.str.contains(\"D2E\"))[0]\n",
    "    steps = len(df)-n_steps+1\n",
    "    steps = 1\n",
    "    for i in range(steps):\n",
    "        y.append(df.values[i+n_steps-1,time_idx])\n",
    "    return np.array(y)\n",
    "\n",
    "def preprocess(dataset, min_case_length = 3, name= \"Helpdesk Dataset\"):\n",
    "    tmp = dataset.groupby([\"CaseID\"]).count()\n",
    "    to_drop = list(tmp.loc[tmp[\"ActivityID\"] < min_case_length].index)\n",
    "    dataset.CompleteTimestamp = pd.to_datetime(dataset.CompleteTimestamp)\n",
    "    dataset.sort_values([\"CaseID\", \"CompleteTimestamp\"],ascending=True)\n",
    "    dataset = dataset.loc[~dataset[\"CaseID\"].isin(to_drop)].reset_index(drop=True)\n",
    "    dataset = dataset.groupby(\"CaseID\").apply(lambda case:time_features(case))\n",
    "    dataset[\"D2E\"] = dataset[\"T2E\"].apply(lambda x:x.days)\n",
    "    dataset[\"H2E\"] = dataset[\"T2E\"].apply(lambda x:x.total_seconds()/3600)\n",
    "    dataset[\"fvt1\"] = dataset[\"fvt1\"].apply(lambda x:x.total_seconds()/3600)\n",
    "    dataset.fvt1.fillna(0,inplace=True)\n",
    "#     dataset[\"weekday\"] = dataset[\"CompleteTimestamp\"].dt.weekday\n",
    "#     dummy1 = pd.get_dummies(dataset[\"weekday\"],prefix=\"weekday\",drop_first=True)\n",
    "    dummy2 = pd.get_dummies(dataset[\"ActivityID\"],prefix=\"ActivityID\",drop_first=True)\n",
    "#     dataset = pd.concat([dataset,dummy1,dummy2],axis=1)\n",
    "    dataset = pd.concat([dataset,dummy2],axis=1)\n",
    "    last_step = dataset.drop_duplicates(subset=[\"CaseID\"],keep='last')[\"ActivityID\"].index\n",
    "    dataset = dataset.drop(last_step,axis=0).reset_index(drop=True)\n",
    "    sc = StandardScaler()\n",
    "    dataset[\"fvt1\"] = sc.fit_transform(dataset[\"fvt1\"].values.reshape(-1,1))\n",
    "    dataset[\"fvt2\"] = sc.fit_transform(dataset[\"fvt2\"].values.reshape(-1,1))\n",
    "    dataset[\"fvt3\"] = sc.fit_transform(dataset[\"fvt3\"].values.reshape(-1,1))\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2e",
   "language": "python",
   "name": "t2e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
